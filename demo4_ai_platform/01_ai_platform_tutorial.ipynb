{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - AI Platform 教程\n",
    "\n",
    "本教程完整演示 AI Platform 的端到端流程：从数据入库、模型训练到推理服务。\n",
    "\n",
    "## 架构回顾\n",
    "\n",
    "AI Platform 由两个微服务组成：\n",
    "\n",
    "| 服务 | 端口 | 职责 |\n",
    "|------|------|------|\n",
    "| **Orchestrator** | 8000 | 用户唯一入口，任务编排、代理查询、推理服务 |\n",
    "| **Executor** | 8001 | 数据湖存储（Lance）+ 脚本执行器（Daft） |\n",
    "\n",
    "两者通过 **Lance 格式**作为契约层连接。用户只与 Orchestrator 交互，不直接接触 Executor。\n",
    "\n",
    "```\n",
    "用户 --> Orchestrator (8000) --> Executor (8001) --> Lance 数据湖\n",
    "```\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "- 理解 Orchestrator + Executor 微服务架构\n",
    "- 通过 HTTP API 完成 MNIST 数据入库、CNN 训练、推理\n",
    "- 观察 Lance 数据湖中的数据集和模型\n",
    "- 可视化推理结果\n",
    "\n",
    "## 前置条件\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 启动服务\n",
    "\n",
    "用 `subprocess` 在后台启动 Executor 和 Orchestrator 两个 uvicorn 进程。\n",
    "\n",
    "启动顺序：先 Executor（8001），再 Orchestrator（8000），因为 Orchestrator 依赖 Executor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "import httpx\n",
    "\n",
    "# 启动 Executor（数据湖 + 脚本执行器）\n",
    "executor_proc = subprocess.Popen(\n",
    "    [\"uvicorn\", \"executor.app:app\", \"--port\", \"8001\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "print(f\"Executor 已启动 (PID: {executor_proc.pid})\")\n",
    "\n",
    "# 启动 Orchestrator（用户入口）\n",
    "orchestrator_proc = subprocess.Popen(\n",
    "    [\"uvicorn\", \"orchestrator.app:app\", \"--port\", \"8000\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "print(f\"Orchestrator 已启动 (PID: {orchestrator_proc.pid})\")\n",
    "\n",
    "# 等待服务就绪\n",
    "time.sleep(3)\n",
    "\n",
    "# 定义 API 基地址（后续所有请求都发到 Orchestrator）\n",
    "BASE_URL = \"http://localhost:8000/api/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证服务是否正常运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 Orchestrator 是否能连接到 Executor\n",
    "resp = httpx.get(f\"{BASE_URL}/datasets\")\n",
    "print(f\"状态码: {resp.status_code}\")\n",
    "print(f\"数据集列表: {resp.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据入库（Ingestion）\n",
    "\n",
    "提交一个 `type=ingestion` 任务，让平台执行我们的清洗脚本 `mnist_clean.py`。\n",
    "\n",
    "脚本会：\n",
    "1. 自动下载 MNIST 数据集（60000 训练 + 10000 测试）\n",
    "2. 将 28x28 图像展平为 784 维向量\n",
    "3. 归一化像素值到 [0, 1]\n",
    "4. 写入 Lance 格式\n",
    "\n",
    "**注意**：首次运行需要下载 MNIST 数据（约 11MB），请耐心等待。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提交数据入库任务\n",
    "resp = httpx.post(f\"{BASE_URL}/tasks\", json={\n",
    "    \"type\": \"ingestion\",\n",
    "    \"name\": \"mnist_ingestion\",\n",
    "    \"input\": \"download\",                                    # 自动下载 MNIST\n",
    "    \"script\": \"scripts/pipelines/mnist_clean.py\",            # 清洗脚本路径\n",
    "    \"params\": {\"normalize\": True},                           # 归一化像素值\n",
    "    \"output\": \"lance_storage/datasets/mnist_clean.lance\",    # 输出到数据湖\n",
    "})\n",
    "\n",
    "ingestion_task = resp.json()\n",
    "print(f\"任务 ID: {ingestion_task['id']}\")\n",
    "print(f\"状态: {ingestion_task['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务在后台异步执行。我们通过轮询 `GET /tasks/{id}` 等待完成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轮询等待任务完成\n",
    "task_id = ingestion_task[\"id\"]\n",
    "while True:\n",
    "    resp = httpx.get(f\"{BASE_URL}/tasks/{task_id}\")\n",
    "    task = resp.json()\n",
    "    status = task[\"status\"]\n",
    "    if status in (\"completed\", \"failed\"):\n",
    "        break\n",
    "    print(f\"状态: {status}，等待中...\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"\\n最终状态: {status}\")\n",
    "if status == \"completed\":\n",
    "    print(f\"结果: {task['result']}\")\n",
    "else:\n",
    "    print(f\"错误: {task.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 查看数据集\n",
    "\n",
    "数据入库完成后，可以通过 `GET /datasets` 查看数据湖中的数据集。\n",
    "\n",
    "注意响应中的 `schema` 字段——这就是 Lance 文件的列定义：\n",
    "- `image`: 784 维浮点数组（展平后的 28x28 像素）\n",
    "- `label`: 0-9 的整数标签\n",
    "- `split`: \"train\" 或 \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有数据集\n",
    "resp = httpx.get(f\"{BASE_URL}/datasets\")\n",
    "datasets = resp.json()\n",
    "\n",
    "for ds in datasets:\n",
    "    print(f\"数据集: {ds['id']}\")\n",
    "    print(f\"  路径: {ds['path']}\")\n",
    "    print(f\"  行数: {ds['num_rows']}\")\n",
    "    print(f\"  Schema:\")\n",
    "    for col, dtype in ds[\"schema\"].items():\n",
    "        print(f\"    {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以查看单个数据集的详情："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 mnist_clean 数据集详情\n",
    "resp = httpx.get(f\"{BASE_URL}/datasets/mnist_clean\")\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练（Training）\n",
    "\n",
    "提交一个 `type=training` 任务，训练一个简单的 CNN 分类器。\n",
    "\n",
    "训练脚本 `mnist_cnn.py` 会：\n",
    "1. 从 Lance 数据湖读取训练数据\n",
    "2. 构建 PyTorch DataLoader\n",
    "3. 训练 CNN（两层卷积 + 两层全连接）\n",
    "4. 评估测试集准确率\n",
    "5. 将模型权重 + 指标保存回 Lance\n",
    "\n",
    "我们用 3 个 epoch 做演示，CPU 上大约需要 1-2 分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提交训练任务\n",
    "resp = httpx.post(f\"{BASE_URL}/tasks\", json={\n",
    "    \"type\": \"training\",\n",
    "    \"name\": \"mnist_cnn_v1\",\n",
    "    \"input\": \"lance_storage/datasets/mnist_clean.lance\",     # 从数据湖读取\n",
    "    \"script\": \"scripts/training/mnist_cnn.py\",               # 训练脚本\n",
    "    \"params\": {\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64,\n",
    "        \"device\": \"cpu\",\n",
    "    },\n",
    "    \"output\": \"lance_storage/models/mnist_cnn_v1.lance\",     # 模型保存到数据湖\n",
    "})\n",
    "\n",
    "training_task = resp.json()\n",
    "print(f\"任务 ID: {training_task['id']}\")\n",
    "print(f\"状态: {training_task['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轮询等待训练完成\n",
    "task_id = training_task[\"id\"]\n",
    "while True:\n",
    "    resp = httpx.get(f\"{BASE_URL}/tasks/{task_id}\")\n",
    "    task = resp.json()\n",
    "    status = task[\"status\"]\n",
    "    if status in (\"completed\", \"failed\"):\n",
    "        break\n",
    "    print(f\"状态: {status}，训练中...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"\\n最终状态: {status}\")\n",
    "if status == \"completed\":\n",
    "    result = task[\"result\"]\n",
    "    print(f\"准确率: {result['accuracy']:.2%}\")\n",
    "    print(f\"测试损失: {result['test_loss']:.4f}\")\n",
    "else:\n",
    "    print(f\"错误: {task.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 查看模型\n",
    "\n",
    "训练完成后，模型权重和元数据都保存在 Lance 数据湖中。\n",
    "\n",
    "Lance 表的 schema：\n",
    "- `weights`: Binary — PyTorch state_dict 的序列化字节\n",
    "- `params`: String — 超参数 JSON\n",
    "- `metrics`: String — 训练指标 JSON\n",
    "- `created_at`: String — 创建时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有模型\n",
    "resp = httpx.get(f\"{BASE_URL}/models\")\n",
    "models = resp.json()\n",
    "\n",
    "for m in models:\n",
    "    print(f\"模型: {m['id']}\")\n",
    "    print(f\"  路径: {m['path']}\")\n",
    "    print(f\"  Schema:\")\n",
    "    for col, dtype in m[\"schema\"].items():\n",
    "        print(f\"    {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 启动推理服务（Inference）\n",
    "\n",
    "提交一个 `type=inference` 任务，Orchestrator 会：\n",
    "1. 通过 Executor API 获取模型的 Lance 文件路径\n",
    "2. 用 Daft 读取 Lance 中的模型权重\n",
    "3. 用 PyTorch 加载权重到 CNN 模型\n",
    "4. 模型进入 eval 模式，准备接收 predict 请求\n",
    "\n",
    "与批处理任务不同，推理任务是**常驻服务**——一直保持 `running` 状态直到手动 `cancel`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动推理服务\n",
    "resp = httpx.post(f\"{BASE_URL}/tasks\", json={\n",
    "    \"type\": \"inference\",\n",
    "    \"name\": \"mnist_predictor\",\n",
    "    \"model\": \"mnist_cnn_v1\",    # 从数据湖加载这个模型\n",
    "    \"device\": \"cpu\",\n",
    "    \"port\": 8080,\n",
    "})\n",
    "\n",
    "inference_task = resp.json()\n",
    "print(f\"任务 ID: {inference_task['id']}\")\n",
    "print(f\"状态: {inference_task['status']}\")\n",
    "print(f\"端点: {inference_task.get('endpoint', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 调用推理\n",
    "\n",
    "推理服务就绪后，通过 `POST /tasks/{id}/predict` 发送图像数据。\n",
    "\n",
    "请求体是 784 维浮点数组（28x28 归一化像素值）。\n",
    "\n",
    "我们从数据湖中取一张真实的测试图片来试试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "# 从数据湖读取测试集中的前 5 张图片\n",
    "df = daft.read_lance(\"lance_storage/datasets/mnist_clean.lance\")\n",
    "pdf = df.to_pandas()\n",
    "test_samples = pdf[pdf[\"split\"] == \"test\"].head(5)\n",
    "\n",
    "print(f\"取出 {len(test_samples)} 张测试图片\")\n",
    "print(f\"真实标签: {test_samples['label'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每张图片调用推理 API\n",
    "inference_id = inference_task[\"id\"]\n",
    "predictions = []\n",
    "\n",
    "for i, row in test_samples.iterrows():\n",
    "    image_data = [float(x) for x in row[\"image\"]]  # 转为 Python float 列表\n",
    "    resp = httpx.post(\n",
    "        f\"{BASE_URL}/tasks/{inference_id}/predict\",\n",
    "        json={\"image\": image_data},\n",
    "    )\n",
    "    result = resp.json()\n",
    "    predictions.append(result)\n",
    "    print(f\"真实: {row['label']}, 预测: {result['prediction']}, 置信度: {result['confidence']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化预测结果\n",
    "\n",
    "将 784 维向量还原为 28x28 图像，展示预测结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for idx, (ax, (_, row)) in enumerate(zip(axes, test_samples.iterrows())):\n",
    "    # 将 784 维向量还原为 28x28 图像\n",
    "    image = np.array(row[\"image\"]).reshape(28, 28)\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    pred = predictions[idx]\n",
    "    true_label = row[\"label\"]\n",
    "    pred_label = pred[\"prediction\"]\n",
    "    confidence = pred[\"confidence\"]\n",
    "\n",
    "    # 预测正确显示绿色，错误显示红色\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    ax.set_title(f\"真实: {true_label}\\n预测: {pred_label} ({confidence:.1%})\", color=color)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"MNIST 推理结果\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概率分布\n",
    "\n",
    "查看第一张图片的 10 类概率分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一张图片的概率分布\n",
    "probs = predictions[0][\"probabilities\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "bars = ax.bar(range(10), probs)\n",
    "\n",
    "# 高亮预测类别\n",
    "pred_idx = predictions[0][\"prediction\"]\n",
    "bars[pred_idx].set_color(\"green\")\n",
    "\n",
    "ax.set_xlabel(\"数字类别\")\n",
    "ax.set_ylabel(\"概率\")\n",
    "ax.set_title(f\"预测概率分布（真实标签: {test_samples.iloc[0]['label']}）\")\n",
    "ax.set_xticks(range(10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 查看所有任务\n",
    "\n",
    "回顾我们创建的所有任务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出所有任务\n",
    "resp = httpx.get(f\"{BASE_URL}/tasks\")\n",
    "tasks = resp.json()\n",
    "\n",
    "for t in tasks:\n",
    "    print(f\"{t['id']}  type={t['type']:<10}  status={t['status']:<10}  name={t['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以按类型过滤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只看推理任务\n",
    "resp = httpx.get(f\"{BASE_URL}/tasks\", params={\"type\": \"inference\"})\n",
    "print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 停止推理服务\n",
    "\n",
    "推理任务是常驻服务，需要手动取消。取消后模型会从内存中卸载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止推理服务\n",
    "resp = httpx.post(f\"{BASE_URL}/tasks/{inference_task['id']}/cancel\")\n",
    "print(f\"推理服务已停止: {resp.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 清理\n",
    "\n",
    "停止 Executor 和 Orchestrator 进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止服务\n",
    "orchestrator_proc.terminate()\n",
    "executor_proc.terminate()\n",
    "orchestrator_proc.wait()\n",
    "executor_proc.wait()\n",
    "print(\"所有服务已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本教程完整演示了 AI Platform 的三种任务类型：\n",
    "\n",
    "| 步骤 | API | 任务类型 | 说明 |\n",
    "|------|-----|---------|------|\n",
    "| 数据入库 | `POST /tasks` | ingestion | 下载 MNIST，归一化，写入 Lance |\n",
    "| 查看数据集 | `GET /datasets` | — | 查看 schema 和行数 |\n",
    "| 模型训练 | `POST /tasks` | training | CNN 训练，权重保存到 Lance |\n",
    "| 查看模型 | `GET /models` | — | 查看模型元数据 |\n",
    "| 启动推理 | `POST /tasks` | inference | 加载模型到内存 |\n",
    "| 调用推理 | `POST /tasks/{id}/predict` | — | 发送图像，获取预测 |\n",
    "| 停止推理 | `POST /tasks/{id}/cancel` | — | 卸载模型 |\n",
    "\n",
    "### 关键设计点\n",
    "\n",
    "- **统一任务 API**：三种任务共用 `/tasks` 端点，通过 `type` 区分\n",
    "- **Lance 作为契约层**：数据集和模型都存储在 Lance 格式中，两个服务通过共享存储交换数据\n",
    "- **用户脚本模式**：平台不绑定特定数据集或模型，用户提供自己的清洗/训练脚本\n",
    "- **微服务解耦**：Orchestrator 不直接操作数据，通过 Executor API 代理\n",
    "\n",
    "### 进阶方向\n",
    "\n",
    "- Level 2: 引入 Ray，支持并发任务和 Ray Serve 推理\n",
    "- Level 3: Ray on K8s + S3 共享存储，多机多任务\n",
    "- 详见 [design.md](./design.md) 中的部署级别设计"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
